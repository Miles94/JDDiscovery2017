{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据表的关联"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据表的关联\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import datetime\n",
    "\n",
    "login = pd.read_csv('../data/t_login.csv')\n",
    "login_test = pd.read_csv('../data/t_login_test.csv')\n",
    "trade = pd.read_csv('../data/t_trade.csv')\n",
    "trade_test = pd.read_csv('../data/t_trade_test.csv')\n",
    "\n",
    "#区分login和trade的time\n",
    "login.rename(columns={'time':'login_time'},inplace=True)\n",
    "login_test.rename(columns={'time':'login_time'},inplace=True)\n",
    "trade.rename(columns={'time':'trade_time'},inplace=True)\n",
    "trade_test.rename(columns={'time':'trade_time'},inplace=True)\n",
    "\n",
    "#时间字符串转换为datetime格式\n",
    "login['login_time'] = pd.to_datetime(login['login_time'])\n",
    "login_test['login_time'] = pd.to_datetime(login_test['login_time'])\n",
    "trade['trade_time'] = pd.to_datetime(trade['trade_time'])\n",
    "trade_test['trade_time'] = pd.to_datetime(trade_test['trade_time'])\n",
    "\n",
    "#过滤掉timelong小于等于0的登录记录\n",
    "login = login[login['timelong'] > 0].reset_index(drop=True)\n",
    "login_test = login_test[login_test['timelong'] > 0].reset_index(drop=True)\n",
    "\n",
    "#存储简单处理过的数据\n",
    "login.to_csv('../data/newdata/login.csv', index=False)\n",
    "login_test.to_csv('../data/newdata/login_test.csv', index=False)\n",
    "trade.to_csv('../data/newdata/trade.csv', index=False)\n",
    "trade_test.to_csv('../data/newdata/trade_test.csv', index=False)\n",
    "\n",
    "#登录表和交易表的训练集连接\n",
    "trade_temp=pd.DataFrame(columns=login.columns)\n",
    "for index in trade.index:\n",
    "    temp=login[(trade.loc[index,'id']==login['id'])&(trade.loc[index,'trade_time']>login['login_time'])&(login['result']==1)]\n",
    "    temp=temp.sort_index(by='id')[-1:]\n",
    "    #def top(df, n, column):\n",
    "    #    return df.sort_index(by=column)[-n:]\n",
    "    #temp=temp.groupby('id',as_index=False).apply(top,n=1,column='login_time')\n",
    "    temp.reset_index(drop=True, inplace=True)\n",
    "    trade_temp = trade_temp.append(temp, ignore_index=True)\n",
    "    trade_temp.loc[index,'rowkey'] = trade.loc[index, 'rowkey']\n",
    "\n",
    "trade_temp.drop('id', axis=1, inplace=True)\n",
    "login_trade = pd.merge(trade, trade_temp, on='rowkey', how='left')\n",
    "login_trade.to_csv('../data/newdata/login_trade.csv', index=None)\n",
    "del temp, trade_temp\n",
    "\n",
    "#登录表和交易表的测试集连接\n",
    "trade_test_temp=pd.DataFrame(columns=login_test.columns)\n",
    "for index in trade_test.index:\n",
    "    temp=login_test[(trade_test.loc[index,'id']==login_test['id'])&(trade_test.loc[index,'trade_time']>login_test['login_time'])&(login_test['result']==1)]\n",
    "    temp=temp.sort_index(by='id')[-1:]\n",
    "    #def top(df, n, column):\n",
    "    #    return df.sort_index(by=column)[-n:]\n",
    "    #temp=temp.groupby('id',as_index=False).apply(top,n=1,column='login_test_time')\n",
    "    temp.reset_index(drop=True, inplace=True)\n",
    "    trade_test_temp = trade_test_temp.append(temp, ignore_index=True)\n",
    "    trade_test_temp.loc[index,'rowkey'] = trade_test.loc[index, 'rowkey']\n",
    "\n",
    "trade_test_temp.drop('id', axis=1, inplace=True)\n",
    "login_trade_test = pd.merge(trade_test, trade_test_temp, on='rowkey', how='left')\n",
    "login_trade_test.to_csv('../data/newdata/login_trade_test.csv', index=None)\n",
    "del temp, trade_test_temp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据的探索与前处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login = pd.read_csv('../data/newdata/login.csv')\n",
    "login_test = pd.read_csv('../data/newdata/login_test.csv')\n",
    "trade = pd.read_csv('../data/newdata/trade.csv')\n",
    "trade_test = pd.read_csv('../data/newdata/trade_test.csv')\n",
    "login_trade = pd.read_csv('../data/newdata/login_trade.csv')\n",
    "login_trade_test = pd.read_csv('../data/newdata/login_trade_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#过滤掉没有登录信息的交易\n",
    "login_trade = login_trade.dropna(axis=0, how='any').reset_index(drop=True)\n",
    "login_trade_test = login_trade_test.dropna(axis=0, how='any').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 时间trade_time和login_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#时间字符串转换为datetime格式\n",
    "login['login_time'] = pd.to_datetime(login['login_time'])\n",
    "login_test['login_time'] = pd.to_datetime(login_test['login_time'])\n",
    "login_trade['login_time'] = pd.to_datetime(login_trade['login_time'])\n",
    "login_trade_test['login_time'] = pd.to_datetime(login_trade_test['login_time'])\n",
    "login_trade['trade_time'] = pd.to_datetime(login_trade['trade_time'])\n",
    "login_trade_test['trade_time'] = pd.to_datetime(login_trade_test['trade_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = login_trade\n",
    "feature_test = login_trade_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#交易时的hour值\n",
    "feature['hour'] = feature['trade_time'].map(lambda x : x.hour)\n",
    "#交易时间与登陆时间的差值\n",
    "feature['delta_time'] = (feature['trade_time']-feature['login_time']).map(lambda x : x.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#每次交易时间的差\n",
    "feature['trade_time_sub'] = feature[['id','trade_time']].sort_values(by='trade_time').groupby('id').diff()['trade_time']\n",
    "feature['trade_time_sub_day'] = feature['trade_time_sub'].map(lambda x : x.total_seconds())\n",
    "feature['trade_time_sub_day'].fillna(0, inplace=True)\n",
    "del feature['trade_time_sub']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试集\n",
    "#交易时的hour值\n",
    "feature_test['hour'] = feature_test['trade_time'].map(lambda x : x.hour)\n",
    "#交易时间与登陆时间的差值\n",
    "feature_test['delta_time'] = (feature_test['trade_time']-feature_test['login_time']).map(lambda x : x.total_seconds())\n",
    "#每次交易时间的差\n",
    "feature_test['trade_time_sub'] = feature_test[['id','trade_time']].sort_values(by='trade_time').groupby('id').diff()['trade_time']\n",
    "feature_test['trade_time_sub_day'] = feature_test['trade_time_sub'].map(lambda x : x.total_seconds())\n",
    "feature_test['trade_time_sub_day'].fillna(0, inplace=True)\n",
    "del feature_test['trade_time_sub']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## city,device,ip,log_from,type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### city,device,ip是否多次变化（针对用户id）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#得到DataFrame中的无重复的id\n",
    "def getUserIDFromDataFrame(dataFrame):\n",
    "    return pd.DataFrame({'id':dataFrame['id'].unique()})\n",
    "\n",
    "#得到某列的分组不同的个数\n",
    "def getCountsByColumnName(loginData,IDdata,columnName):\n",
    "    col_data = login[columnName].groupby(login['id']).nunique().reset_index()\n",
    "    col_data.rename(columns={columnName : columnName + '_count'}, inplace=True)\n",
    "    IDdata = pd.merge(IDdata, col_data, on='id', how='left')\n",
    "    return IDdata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#每个id登录成功记录对所有登录记录所占的比例\n",
    "def getResultCount(loginData, IDdata):\n",
    "    col_data = loginData[['id', 'result']].groupby('id').count().reset_index()\n",
    "    loginData['result_1'] = loginData['result']==1\n",
    "    col_data2 = loginData[['id', 'result_1']].groupby('id').sum().reset_index()\n",
    "    col_data['result_rate'] = col_data2['result_1']/col_data['result']\n",
    "    loginData.drop('result_1',axis=1,inplace=True)\n",
    "    col_data.drop('result',axis=1,inplace=True)\n",
    "    del col_data2\n",
    "    IDdata = pd.merge(IDdata, col_data, on='id', how='left')\n",
    "    return IDdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDdata = getUserIDFromDataFrame(login)\n",
    "IDdata = getCountsByColumnName(login, IDdata, 'city')\n",
    "IDdata = getCountsByColumnName(login, IDdata, 'device')\n",
    "IDdata = getCountsByColumnName(login, IDdata, 'ip')\n",
    "\n",
    "IDdata = getResultCount(login, IDdata)\n",
    "\n",
    "feature = pd.merge(feature, IDdata, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDdata = getUserIDFromDataFrame(login_test)\n",
    "IDdata = getCountsByColumnName(login_test, IDdata, 'city')\n",
    "IDdata = getCountsByColumnName(login_test, IDdata, 'device')\n",
    "IDdata = getCountsByColumnName(login_test, IDdata, 'ip')\n",
    "\n",
    "IDdata = getResultCount(login_test, IDdata)\n",
    "\n",
    "feature_test = pd.merge(feature_test, IDdata, on='id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交易表中的city，device，IP，log_from，type是否为登录表中用户最常用的类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#选取指定列具有最大值的行的函数\n",
    "def top(df, n, column):\n",
    "    return df.sort_index(by=column)[-n:]\n",
    "\n",
    "def get_commontype(loginData, feature, columnName):\n",
    "    col_data = loginData[['id',columnName,'log_id']].groupby(['id',columnName]).count()\n",
    "    col_data.reset_index(inplace=True)\n",
    "    col_data.rename(columns = {columnName : 'commontype_'+columnName}, inplace=True)\n",
    "    col_data = col_data.groupby('id', as_index=False).apply(top, n=1, column='log_id').reset_index(drop=True)\n",
    "    col_data.drop('log_id', axis=1, inplace=True)\n",
    "    feature = pd.merge(feature, col_data, on='id', how='left')\n",
    "    feature['is_common_'+columnName] = (feature[columnName]==feature['commontype_'+columnName]).astype(int)\n",
    "    feature.drop('commontype_'+columnName, axis=1, inplace=True)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#交易表中的city，device，IP，log_from，type是否为登录表中用户最常用的类型\n",
    "feature = get_commontype(login, feature, 'city')\n",
    "feature = get_commontype(login, feature, 'device')\n",
    "feature = get_commontype(login, feature, 'ip')\n",
    "feature = get_commontype(login, feature, 'log_from')\n",
    "feature = get_commontype(login, feature, 'type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#交易表中的city，device，IP，log_from，type是否为登录表中用户最常用的类型\n",
    "feature_test = get_commontype(login_test, feature_test, 'city')\n",
    "feature_test = get_commontype(login_test, feature_test, 'device')\n",
    "feature_test = get_commontype(login_test, feature_test, 'ip')\n",
    "feature_test = get_commontype(login_test, feature_test, 'log_from')\n",
    "feature_test = get_commontype(login_test, feature_test, 'type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 对log_from,type进行one-hot处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对log_from,type进行one-hot处理\n",
    "feature = pd.get_dummies(feature, columns=['log_from','type'], prefix=['log_from','type'])\n",
    "feature_test = pd.get_dummies(feature_test, columns=['log_from','type'], prefix=['log_from','type'])\n",
    "feature.drop('log_from_18.0', axis=1, inplace=True)\n",
    "#feature_test中不含log_from_18.0，且feature中只有7个log_from_18.0，故去掉"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将is_scan,is_sec的bool值改为0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将is_scan,is_sec的bool值改为0/1\n",
    "feature[['is_scan', 'is_sec']] = feature[['is_scan', 'is_sec']].astype(int)\n",
    "feature_test[['is_scan', 'is_sec']] = feature_test[['is_scan', 'is_sec']].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 删除无用的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除无用的特征\n",
    "feature.drop(['log_id', 'login_time', 'result', 'timelong', 'timestamp'], axis=1, inplace=True)\n",
    "feature_test.drop(['log_id', 'login_time', 'result', 'timelong', 'timestamp'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存特征文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature.to_csv('../feature/feature001.csv',index=None)\n",
    "feature_test.to_csv('../feature/feature_test001.csv',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型求解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读入数据\n",
    "feature_csv = pd.read_csv('../feature/feature001.csv')\n",
    "feature_test_csv = pd.read_csv('../feature/feature_test001.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#选取训练集和测试集\n",
    "train_raw = feature_csv.drop(['rowkey', 'trade_time', 'id','city','device','ip'], axis=1)\n",
    "test = feature_test_csv.drop(['rowkey', 'trade_time', 'id','city','device','ip'], axis=1)\n",
    "rowkeytest = feature_test_csv['rowkey']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"neg:{0},pos:{1}\".format(len(train_raw[train_raw['is_risk']==0]),len(train_raw[train_raw['is_risk']==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#划分训练集与验证集\n",
    "train,val = train_test_split(train_raw, test_size = 0.2,random_state=1)\n",
    "y = train['is_risk']\n",
    "X = train.drop(['is_risk'],axis=1)\n",
    "val_y = val['is_risk']\n",
    "val_X = val.drop(['is_risk'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#自定义评价函数\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def customedscore(preds, dtrain):\n",
    "    label = dtrain.get_label()\n",
    "    pred = [int(i>=0.5) for i in preds]\n",
    "    confusion_matrixs = confusion_matrix(label, pred)\n",
    "    recall =float(confusion_matrixs[1][1]) / float(confusion_matrixs[1][0]+confusion_matrixs[1][1])\n",
    "    precision = float(confusion_matrixs[1][1]) / float(confusion_matrixs[0][1]+confusion_matrixs[1][1])\n",
    "    F = (1+0.01)*precision* recall/(0.01*precision+recall)\n",
    "    return 'FSCORE',float(F)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost start here\n",
    "dtest = xgb.DMatrix(test)\n",
    "dval = xgb.DMatrix(val_X,label=val_y)\n",
    "dtrain = xgb.DMatrix(X, label=y)\n",
    "params={\n",
    "    'booster':'gbtree',\n",
    "    'objective': 'binary:logistic',\n",
    "    'early_stopping_rounds':100,\n",
    "    'scale_pos_weight': 3570/116165,\n",
    "    'gamma':0.1,#0.2 is ok\n",
    "    'eval_metric': 'auc',\n",
    "    'max_depth':8,\n",
    "    'lambda':550,\n",
    "    'subsample':0.7,\n",
    "    'colsample_bytree':0.3,\n",
    "    'min_child_weight':3, \n",
    "    'eta': 0.007,\n",
    "    'seed':random_seed\n",
    "    }\n",
    "\n",
    "watchlist  = [(dtrain,'train'),(dval,'val')]#The early stopping is based on last set in the evallist\n",
    "model = xgb.train(params,dtrain,num_boost_round=10000,evals=watchlist)\n",
    "model.save_model('../model/xgb.model')\n",
    "print \"best best_ntree_limit\",model.best_ntree_limit   #did not save the best,why?\n",
    "\n",
    "#predict test set (from the best iteration)\n",
    "test_y = model.predict(dtest,ntree_limit=model.best_ntree_limit)\n",
    "test_result = pd.DataFrame(columns=[\"rowkey\",\"score\"])\n",
    "test_result.rowkey = rowkeytest\n",
    "test_result.score = test_y\n",
    "test_result.to_csv(\"../xgb/xgb.csv\",index=None,encoding='utf-8')  #remember to edit xgb.csv , add \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = pd.read_csv(\"../xgb/xgb.csv\")\n",
    "test_result.score = test_result.score.map(lambda i: int(i>=0.6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_test = pd.read_csv('../data/t_trade_test.csv')\n",
    "trade = pd.read_csv('../data/t_trade.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb001 = pd.merge(trade_test[['rowkey']], test_result, on='rowkey', how='left')\n",
    "xgb001 = xgb001.fillna(0)\n",
    "xgb001['score']=xgb001['score'].astype('int')\n",
    "xgb001.to_csv(\"../xgb/xgb001.csv\",header=False,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "526px",
    "left": "0px",
    "right": "1262px",
    "top": "111px",
    "width": "104px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
